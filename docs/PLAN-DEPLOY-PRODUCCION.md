# üöÄ Plan de Deployment a Producci√≥n - FixeatAI

**Fecha:** 26 de Enero, 2026  
**Estado:** Pendiente de datos de m√°quina

---

## üìã **Checklist de Deployment**

### **Fase 1: Preparaci√≥n (Local)** ‚úÖ
- [x] Sistema funcional en local
- [x] 67 PDFs ingresados en ChromaDB
- [x] B√∫squeda h√≠brida implementada
- [x] Scoring de relevancia sin alucinaciones
- [x] API con 6 endpoints funcionales
- [x] Frontend de pruebas
- [x] Documentaci√≥n completa

### **Fase 2: Configuraci√≥n de Infraestructura** ‚è≥
- [ ] Obtener datos de instancia EC2
- [ ] Configurar Security Groups (puertos 22, 8080, 7070, 9000)
- [ ] Configurar EBS para persistencia (50 GB m√≠nimo)
- [ ] Configurar Elastic IP (opcional pero recomendado)
- [ ] Configurar IAM Role para S3 (backups)

### **Fase 3: Preparaci√≥n de Archivos** ‚è≥
- [ ] Crear `.env` de producci√≥n con credenciales reales
- [ ] Configurar docker-compose.prod.yml
- [ ] Preparar backup de ChromaDB local (67 PDFs)
- [ ] Comprimir c√≥digo para subir a EC2

### **Fase 4: Instalaci√≥n en Servidor** ‚è≥
- [ ] Conectar a EC2 via SSH
- [ ] Instalar Docker y Docker Compose
- [ ] Crear directorios de persistencia
- [ ] Subir c√≥digo y configuraci√≥n
- [ ] Subir backup de ChromaDB

### **Fase 5: Configuraci√≥n de Servicios** ‚è≥
- [ ] Configurar variables de entorno
- [ ] Configurar vol√∫menes Docker
- [ ] Configurar reinicio autom√°tico
- [ ] Configurar logs persistentes

### **Fase 6: Deploy y Verificaci√≥n** ‚è≥
- [ ] Build de im√°genes Docker
- [ ] Levantar servicios
- [ ] Health checks de API, MCP, ETL
- [ ] Verificar conectividad a RDS
- [ ] Verificar persistencia de ChromaDB
- [ ] Probar b√∫squeda en KB
- [ ] Probar endpoints de API

### **Fase 7: Configuraci√≥n de Backups** ‚è≥
- [ ] Script de backup de ChromaDB a S3
- [ ] Cron job para backups diarios
- [ ] Verificar backup exitoso
- [ ] Documentar proceso de restore

### **Fase 8: Monitoreo y Alertas** ‚è≥
- [ ] Configurar logs centralizados
- [ ] Configurar m√©tricas (Prometheus opcional)
- [ ] Configurar alertas de salud
- [ ] Documentar URLs de acceso

---

## üîß **Configuraci√≥n Espec√≠fica**

### **1. Variables de Entorno de Producci√≥n**

Crear `/srv/fixeatAI/.env`:

```bash
# =================================================================
# CONFIGURACI√ìN LLM
# =================================================================
OPENAI_API_KEY=sk-proj-XXXXX  # ‚ö†Ô∏è COMPLETAR
LLM_MODEL=gpt-4o-mini
USE_LLM=true

# =================================================================
# CONFIGURACI√ìN MCP
# =================================================================
MCP_SERVER_URL=http://mcp:7000
CHROMA_PATH=/app/chroma_data

# =================================================================
# CONFIGURACI√ìN API
# =================================================================
API_PORT=8080  # Cambiado de 8000 para evitar conflictos
CORS_ORIGINS=*  # ‚ö†Ô∏è Cambiar en producci√≥n a dominio espec√≠fico

# =================================================================
# CONFIGURACI√ìN ETL (Si se usa)
# =================================================================
ETL_DB_HOST=db-dev-requisition.cluster-cwrwuyokixuk.us-east-1.rds.amazonaws.com
ETL_DB_PORT=3306
ETL_DB_USER=admin
ETL_DB_PASSWORD=gXT5a1R2TWtDfR1p7Iwv  # ‚ö†Ô∏è Ya configurado
ETL_DB_DATABASE=requisicion_db
ETL_DB_SSL=true

# =================================================================
# CONFIGURACI√ìN S3 (Backups)
# =================================================================
S3_BUCKET=desa-aibo-wp  # ‚ö†Ô∏è Ya existe
S3_REGION=us-east-1
S3_KB_PREFIX=fixeatAI/kb_backups

# =================================================================
# CONFIGURACI√ìN DE LOGS
# =================================================================
LOG_LEVEL=INFO
LOG_FORMAT=json
```

---

### **2. Docker Compose para Producci√≥n**

Crear `docker-compose.prod.yml`:

```yaml
version: '3.8'

services:
  mcp:
    image: fixeatai:latest
    container_name: fixeatai-mcp-prod
    command: >
      sh -c "if [ \"$$SERVICE_TYPE\" = 'mcp' ]; then
               uvicorn mcp.server_demo:app --host 0.0.0.0 --port 7000;
             fi"
    environment:
      - SERVICE_TYPE=mcp
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLM_MODEL=${LLM_MODEL:-gpt-4o-mini}
      - CHROMA_PATH=/app/chroma_data
    volumes:
      - /srv/fixeatAI/chroma_data:/app/chroma_data:rw
      - /srv/fixeatAI/logs:/app/logs:rw
    ports:
      - "7070:7000"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - fixeatai-network

  api:
    image: fixeatai:latest
    container_name: fixeatai-api-prod
    command: >
      sh -c "if [ \"$$SERVICE_TYPE\" = 'api' ]; then
               uvicorn app.main:app --host 0.0.0.0 --port 8000;
             fi"
    environment:
      - SERVICE_TYPE=api
      - MCP_SERVER_URL=http://mcp:7000
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - USE_LLM=${USE_LLM:-true}
      - CORS_ORIGINS=${CORS_ORIGINS:-*}
    ports:
      - "8080:8000"  # Externo:Interno
    depends_on:
      - mcp
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - fixeatai-network

  etl-service:
    image: fixeatai-etl:latest
    container_name: fixeatai-etl-prod
    environment:
      - ETL_DB_HOST=${ETL_DB_HOST}
      - ETL_DB_PORT=${ETL_DB_PORT}
      - ETL_DB_USER=${ETL_DB_USER}
      - ETL_DB_PASSWORD=${ETL_DB_PASSWORD}
      - ETL_DB_DATABASE=${ETL_DB_DATABASE}
      - ETL_DB_SSL=${ETL_DB_SSL:-true}
      - ETL_LLM_PROVIDER=${ETL_LLM_PROVIDER:-openai}
      - ETL_LLM_API_KEY=${OPENAI_API_KEY}
      - ETL_LLM_MODEL=${LLM_MODEL:-gpt-4o-mini}
    ports:
      - "9000:9000"
    volumes:
      - /srv/fixeatAI/etl_logs:/app/logs:rw
      - /srv/fixeatAI/etl_configs:/app/configs:rw
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - fixeatai-network

networks:
  fixeatai-network:
    driver: bridge

volumes:
  chroma_data:
  etl_logs:
  etl_configs:
```

---

### **3. Script de Backup a S3**

Crear `/usr/local/bin/fixeatai-backup.sh`:

```bash
#!/usr/bin/env bash
set -euo pipefail

# Configuraci√≥n
DATA_DIR="/srv/fixeatAI/chroma_data"
S3_BUCKET="desa-aibo-wp"
S3_PREFIX="fixeatAI/kb_backups"
STAMP=$(date +%Y%m%d_%H%M%S)
TMP_DIR="/tmp"
BACKUP_FILE="${TMP_DIR}/kb_backup_${STAMP}.tar.gz"
LOG_FILE="/var/log/fixeatai-backup.log"

# Funci√≥n de logging
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

log "üöÄ Iniciando backup de ChromaDB..."

# Crear backup
log "üì¶ Comprimiendo datos de ChromaDB..."
tar -czf "$BACKUP_FILE" -C "$DATA_DIR" . 2>&1 | tee -a "$LOG_FILE"

# Verificar tama√±o
BACKUP_SIZE=$(du -h "$BACKUP_FILE" | cut -f1)
log "üìä Tama√±o del backup: $BACKUP_SIZE"

# Subir a S3
log "‚òÅÔ∏è  Subiendo a S3..."
aws s3 cp "$BACKUP_FILE" "s3://${S3_BUCKET}/${S3_PREFIX}/kb_backup_${STAMP}.tar.gz" \
    --region us-east-1 2>&1 | tee -a "$LOG_FILE"

# Verificar upload
if [ $? -eq 0 ]; then
    log "‚úÖ Backup completado exitosamente: kb_backup_${STAMP}.tar.gz"
else
    log "‚ùå Error al subir backup a S3"
    exit 1
fi

# Limpiar backups locales antiguos (m√°s de 3 d√≠as)
log "üßπ Limpiando backups locales antiguos..."
find "$TMP_DIR" -maxdepth 1 -name 'kb_backup_*.tar.gz' -mtime +3 -delete

# Limpiar backups en S3 antiguos (m√°s de 30 d√≠as) - OPCIONAL
# aws s3 ls "s3://${S3_BUCKET}/${S3_PREFIX}/" | \
#     while read -r line; do
#         createDate=$(echo $line | awk '{print $1" "$2}')
#         createDate=$(date -d"$createDate" +%s)
#         olderThan=$(date -d"-30 days" +%s)
#         if [[ $createDate -lt $olderThan ]]; then
#             fileName=$(echo $line | awk '{print $4}')
#             aws s3 rm "s3://${S3_BUCKET}/${S3_PREFIX}/${fileName}"
#             log "üóëÔ∏è  Eliminado backup antiguo: $fileName"
#         fi
#     done

log "‚úÖ Proceso de backup finalizado"
```

**Hacer ejecutable y configurar cron:**
```bash
sudo chmod +x /usr/local/bin/fixeatai-backup.sh

# Backup diario a las 3 AM
echo "0 3 * * * /usr/local/bin/fixeatai-backup.sh" | sudo tee /etc/cron.d/fixeatai-backup
```

---

### **4. Script de Restore desde S3**

Crear `/usr/local/bin/fixeatai-restore.sh`:

```bash
#!/usr/bin/env bash
set -euo pipefail

S3_BUCKET="desa-aibo-wp"
S3_PREFIX="fixeatAI/kb_backups"
DATA_DIR="/srv/fixeatAI/chroma_data"
TMP_DIR="/tmp"

# Listar backups disponibles
echo "üìã Backups disponibles en S3:"
aws s3 ls "s3://${S3_BUCKET}/${S3_PREFIX}/" --region us-east-1

# Obtener el m√°s reciente
LATEST=$(aws s3 ls "s3://${S3_BUCKET}/${S3_PREFIX}/" --region us-east-1 | \
         sort | tail -1 | awk '{print $4}')

echo ""
echo "üîÑ √öltimo backup: $LATEST"
read -p "¬øDeseas restaurar este backup? (y/n): " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "‚ùå Restore cancelado"
    exit 1
fi

# Descargar backup
echo "‚¨áÔ∏è  Descargando backup desde S3..."
aws s3 cp "s3://${S3_BUCKET}/${S3_PREFIX}/${LATEST}" \
          "${TMP_DIR}/${LATEST}" --region us-east-1

# Detener servicios
echo "üõë Deteniendo servicios..."
cd /srv/fixeatAI
sudo docker-compose -f docker-compose.prod.yml stop mcp

# Limpiar datos actuales
echo "üóëÔ∏è  Limpiando datos actuales..."
sudo rm -rf ${DATA_DIR}/*

# Restaurar backup
echo "üì¶ Extrayendo backup..."
sudo tar -xzf "${TMP_DIR}/${LATEST}" -C "$DATA_DIR"

# Ajustar permisos
sudo chown -R 1000:1000 "$DATA_DIR"

# Reiniciar servicios
echo "üöÄ Reiniciando servicios..."
sudo docker-compose -f docker-compose.prod.yml start mcp

# Verificar
sleep 5
curl -f http://localhost:7070/health && echo "‚úÖ MCP Service restaurado correctamente"

# Limpiar
rm "${TMP_DIR}/${LATEST}"
echo "‚úÖ Restore completado exitosamente"
```

---

## üìä **Comandos de Gesti√≥n en Producci√≥n**

```bash
# Conectar a la instancia
ssh -i tu-key.pem ubuntu@TU-EC2-IP

# Ver servicios
cd /srv/fixeatAI
sudo docker-compose -f docker-compose.prod.yml ps

# Ver logs
sudo docker-compose -f docker-compose.prod.yml logs -f api
sudo docker-compose -f docker-compose.prod.yml logs -f mcp
sudo docker-compose -f docker-compose.prod.yml logs -f etl-service

# Reiniciar servicios
sudo docker-compose -f docker-compose.prod.yml restart

# Actualizar c√≥digo
sudo docker-compose -f docker-compose.prod.yml down
sudo docker-compose -f docker-compose.prod.yml build --no-cache
sudo docker-compose -f docker-compose.prod.yml --env-file .env up -d

# Verificar salud
curl http://localhost:8080/health  # API
curl http://localhost:7070/health  # MCP
curl http://localhost:9000/health  # ETL

# Ver uso de recursos
docker stats

# Ver espacio en disco
df -h
```

---

## üîç **URLs de Acceso Post-Deploy**

Una vez desplegado, el sistema estar√° disponible en:

```
‚úÖ API Principal:    http://TU-EC2-IP:8080
‚úÖ Documentaci√≥n:    http://TU-EC2-IP:8080/docs
‚úÖ Health Check:     http://TU-EC2-IP:8080/health

‚úÖ MCP Server:       http://TU-EC2-IP:7070
‚úÖ MCP Health:       http://TU-EC2-IP:7070/health

‚úÖ ETL Service:      http://TU-EC2-IP:9000
‚úÖ ETL Docs:         http://TU-EC2-IP:9000/docs
```

---

## ‚ö†Ô∏è **Consideraciones de Seguridad para Producci√≥n**

### **1. Cambiar CORS:**
```python
# app/main.py
CORS_ORIGINS = "https://tu-dominio.com"  # No usar "*" en producci√≥n
```

### **2. Configurar HTTPS:**
- Usar ALB (Application Load Balancer) con certificado SSL
- O configurar nginx como reverse proxy con Let's Encrypt

### **3. Restringir Security Groups:**
```
‚ùå NO permitir 0.0.0.0/0 en puertos 7070, 9000
‚úÖ Permitir solo VPC interna o IPs espec√≠ficas
‚úÖ API (8080) detr√°s de ALB con HTTPS
```

### **4. Usar Secrets Manager:**
```bash
# En lugar de .env con API keys en texto plano:
aws secretsmanager create-secret \
    --name fixeatai/openai-key \
    --secret-string "sk-proj-XXXXX"

# Modificar docker-compose para leer de Secrets Manager
```

---

## ‚úÖ **Pr√≥ximos Pasos Inmediatos**

1. **Proveer datos de instancia EC2:**
   - IP p√∫blica o DNS
   - SSH key (.pem)
   - Usuario (ubuntu / ec2-user)

2. **Confirmar credenciales:**
   - OpenAI API Key
   - AWS Access Key (para S3)

3. **Ejecutar deployment**

---

**Estado:** ‚è≥ Esperando datos de infraestructura  
**Tiempo estimado de deploy:** 30-45 minutos una vez tengamos los datos
