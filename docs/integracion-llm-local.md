# ü§ñ Integraci√≥n LLM Local con FixeatAI

## üéØ **Sistema Escalable para LLM Local**

El sistema de auto-taxonom√≠a est√° **100% preparado** para modelos LLM locales. Te muestro las opciones y configuraciones.

## üîß **Opci√≥n 1: Ollama (Recomendado)**

### **Instalaci√≥n Ollama**

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Descargar desde https://ollama.ai/download
```

### **Modelos Recomendados para Taxonom√≠a**

```bash
# Modelos optimizados para an√°lisis t√©cnico
ollama pull llama3.1:8b        # Equilibrio rendimiento/calidad
ollama pull mistral:7b         # M√°s r√°pido, menor memoria
ollama pull codellama:13b      # Especializado en texto t√©cnico
ollama pull llama3.1:70b       # M√°xima calidad (requiere GPU potente)
```

### **Configuraci√≥n en FixeatAI**

```bash
# .env
LLM_AGENTS='{
  "taxonomy": {
    "model": "llama3.1:8b",
    "base_url": "http://localhost:11434/v1",
    "api_key": "sk-local",
    "temperature": 0.1,
    "max_tokens": 1000
  },
  "prediction": {
    "model": "mistral:7b",
    "base_url": "http://localhost:11434/v1",
    "api_key": "sk-local",
    "temperature": 0.2,
    "max_tokens": 800
  }
}'
```

### **Iniciar Ollama**

```bash
# Terminal 1: Servidor Ollama
ollama serve

# Terminal 2: Probar modelo
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.1:8b",
    "messages": [{"role": "user", "content": "¬øQu√© es una laminadora SINMAG?"}]
  }'
```

## üîß **Opci√≥n 2: LocalAI**

### **Docker con LocalAI**

```yaml
# docker-compose-local-ai.yml
version: '3.8'
services:
  localai:
    image: quay.io/go-skynet/local-ai:latest
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models
    environment:
      - THREADS=4
      - CONTEXT_SIZE=1024
    command: --models-path /models --address 0.0.0.0:8080
```

```bash
# Iniciar LocalAI
docker-compose -f docker-compose-local-ai.yml up -d

# Configuraci√≥n en .env
LLM_AGENTS='{
  "taxonomy": {
    "model": "ggml-model-q4_0.bin",
    "base_url": "http://localhost:8080/v1",
    "api_key": "sk-local"
  }
}'
```

## üîß **Opci√≥n 3: LM Studio**

### **Configuraci√≥n LM Studio**

1. **Descargar**: https://lmstudio.ai/
2. **Cargar modelo**: Llama 3.1 8B o Mistral 7B
3. **Iniciar servidor local**: Puerto 1234 (default)

```bash
# .env para LM Studio
LLM_AGENTS='{
  "taxonomy": {
    "model": "llama-3.1-8b-instruct",
    "base_url": "http://localhost:1234/v1",
    "api_key": "sk-local"
  }
}'
```

## üéØ **Puntos de Integraci√≥n en FixeatAI**

### **1. Auto-Taxonom√≠a (Principal)**

```python
# services/taxonomy/auto_learner.py
def _validate_high_frequency_with_llm(self, candidates, corpus_sample):
    """Aqu√≠ se llama al LLM local para validar entidades."""
    
    system_prompt = """
    Eres un experto en equipos industriales de panader√≠a.
    Valida estas entidades extra√≠das de documentos t√©cnicos...
    """
    
    # Esta l√≠nea usa el LLM configurado
    raw_response = self.llm.complete_json(system_prompt, user_prompt)
```

### **2. Habilitaci√≥n Autom√°tica**

```python
# mcp/server_demo.py - Bootstrap
if self.llm is None:
    print("üîß Usando validaci√≥n heur√≠stica (sin LLM)")
    validated = self._fallback_validation(consolidated)
else:
    print("ü§ñ Usando validaci√≥n LLM")
    validated = self._validate_high_frequency_with_llm(consolidated, safe_corpus)
```

### **3. Configuraci√≥n Din√°mica**

```python
# services/llm/client.py - Ya implementado
def __init__(self, agent: Optional[str] = None):
    """Cliente que soporta m√∫ltiples agentes con configs diferentes."""
    
    # Busca configuraci√≥n espec√≠fica del agente
    cfg = agents_cfg.get(agent or "", {})
    
    # Permite base_url para servidores locales
    base_url = cfg.get("base_url")
    if base_url:
        self._client = OpenAI(api_key=api_key, base_url=base_url)
```

## üöÄ **Prueba Inmediata**

### **1. Instalar Ollama y Modelo**

```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Descargar modelo
ollama pull llama3.1:8b

# Iniciar servidor
ollama serve
```

### **2. Configurar FixeatAI**

```bash
# Agregar a .env
echo 'LLM_AGENTS={"taxonomy":{"model":"llama3.1:8b","base_url":"http://localhost:11434/v1","api_key":"sk-local","temperature":0.1}}' >> .env
```

### **3. Probar Bootstrap con LLM**

```bash
# Reiniciar servidor FixeatAI
make mcp

# Probar bootstrap (ahora con LLM local)
curl -X POST http://localhost:7070/tools/taxonomy/bootstrap
```

## üìä **Comparaci√≥n de Opciones**

| Opci√≥n | Facilidad | Rendimiento | Memoria RAM | GPU | Recomendado |
|--------|-----------|-------------|-------------|-----|-------------|
| **Ollama** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 8GB+ | Opcional | ‚úÖ **S√ç** |
| LocalAI | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 6GB+ | Opcional | Para Docker |
| LM Studio | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 8GB+ | Recomendada | Para GUI |

## üéØ **Beneficios del LLM Local**

### **Con LLM Local**
- ‚úÖ **Validaci√≥n inteligente** de entidades candidatas
- ‚úÖ **Detecci√≥n de sin√≥nimos** autom√°tica
- ‚úÖ **Filtrado de falsos positivos** mejorado
- ‚úÖ **Comprensi√≥n contextual** avanzada
- ‚úÖ **Cero costos** de API externa
- ‚úÖ **Privacidad total** de datos

### **Solo Heur√≠stico (Actual)**
- ‚úÖ **R√°pido y eficiente**
- ‚úÖ **Sin dependencias externas**
- ‚ùå Menos preciso en casos ambiguos
- ‚ùå No detecta sin√≥nimos autom√°ticamente

## üîç **Casos de Uso Espec√≠ficos**

### **Detecci√≥n Mejorada con LLM**

**Entrada:**
```
"Se revisa EQUIPO MARCA_NUEVA MOD. MODELO_RARO con problemas"
```

**Heur√≠stico solo:**
- Puede no detectar "MARCA_NUEVA" si no coincide con patrones

**Con LLM Local:**
- Analiza contexto: "EQUIPO" + "MOD." ‚Üí Probablemente marca/modelo
- Valida si "MARCA_NUEVA" es fabricante conocido
- Sugiere sin√≥nimos y variaciones

### **Validaci√≥n Contextual**

**LLM eval√∫a:**
```json
{
  "candidate": "FABRICANTE_DESCONOCIDO",
  "context": "reparaci√≥n de laminadora",
  "llm_decision": "rechazar - no es fabricante conocido",
  "confidence": 0.95
}
```

## üéØ **Configuraci√≥n Recomendada para Producci√≥n**

```bash
# .env - Configuraci√≥n optimizada
LLM_AGENTS='{
  "taxonomy": {
    "model": "llama3.1:8b",
    "base_url": "http://localhost:11434/v1",
    "api_key": "sk-local",
    "temperature": 0.1,
    "max_tokens": 1000
  },
  "prediction": {
    "model": "mistral:7b",
    "base_url": "http://localhost:11434/v1", 
    "api_key": "sk-local",
    "temperature": 0.2,
    "max_tokens": 800
  },
  "router": {
    "model": "llama3.1:8b",
    "base_url": "http://localhost:11434/v1",
    "api_key": "sk-local",
    "temperature": 0.0,
    "max_tokens": 200
  }
}'

# Hardware m√≠nimo recomendado
# - RAM: 16GB (para llama3.1:8b)
# - CPU: 8+ cores
# - GPU: Opcional (acelera procesamiento)
```

**¬°El sistema est√° listo para LLM local sin cambios de c√≥digo!** üöÄ
